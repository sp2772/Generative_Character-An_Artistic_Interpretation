# Generative-Character-An-Artistic-Interpretation

# Vivian (Zenless Zone Zero) - GAN Portrait Generation

This repository contains a from-scratch GAN implementation trained to generate unique, artistic portraits of the character "Vivian" from the game Zenless Zone Zero.

The goal was to develop a model that could create a consistent and compelling art style. The network successfully converged on a painterly aesthetic, learning to render the character's key features with an impressionistic quality.



---

## Table of Contents

* [Project Goal & Overview](#project-goal--overview)
* [Data Acquisition (Pixiv Scraping)](#data-acquisition-pixiv-scraping)
    * [Getting a Pixiv Refresh Token](#getting-a-pixiv-refresh-token)
    * [Downloading Images and Metadata](#downloading-images-and-metadata)
* [Data Preprocessing](#data-preprocessing)
    * [SFW/NSFW Filtering](#sfwnsfw-filtering)
    * [Face Detection and Cropping (YOLOv3)](#face-detection-and-cropping-yolov3)
* [DCGAN Implementation](#dcgan-implementation)
    * [Architecture Overview](#architecture-overview)
    * [Generator](#generator-architecture)
    * [Discriminator](#discriminator-architecture)
* [Training Process](#training-process)
    * [Data Loading & Preprocessing](#data-loading-and-preprocessing)
    * [Training Loop Details](#training-loop-details)
    * [Hyperparameters & Variations](#hyperparameters)
* [Results & Sample Outputs](#results--sample-outputs)
* [How to Use](#how-to-use)
* [Code Structure](#code-structure)
* [References & Tools](#references--tools)

---

## Project Goal & Overview

The primary objective was to train a Deep Convolutional Generative Adversarial Network (DCGAN) capable of generating novel, high-quality artistic portraits of Vivian. The process involved several stages:

1.  **Scraping Data:** Downloading a comprehensive set of images featuring Vivian, along with their associated metadata (tags), from Pixiv.
2.  **Filtering Data:** Separating Safe-For-Work (SFW) images from Not-Safe-For-Work (NSFW) ones based on Pixiv tags.
3.  **Detecting & Cropping Faces:** Using a pre-trained YOLOv3 model specialized for anime heads to isolate Vivian's face from both SFW and NSFW images. This ensures the GAN focuses solely on learning facial features and artistic style.
4.  **Training the DCGAN:** Feeding the cropped facial images into the DCGAN for training, allowing it to learn the underlying patterns and generate new faces in a similar, yet unique, painterly style.

---

## Data Acquisition (Pixiv Scraping)

Images and metadata were sourced from Pixiv using the `pixivpy` library. This requires authentication via a refresh token.

### Getting a Pixiv Refresh Token

To use the Pixiv API, you need a `refresh_token`. The `gppt` tool simplifies this process.

**Reference:** [https://github.com/eggplants/get-pixivpy-token](https://github.com/eggplants/get-pixivpy-token)

**Prerequisites:**
* Google Chrome browser installed.
* Selenium and the appropriate WebDriver for your Chrome version.

**Steps:**

1.  **Install `gppt`:**
    ```bash
    pip install gppt
    ```
2.  **Run the login command (Interactive):**
    ```bash
    gppt login
    ```
    * This will launch a Chrome browser window.
    * Log in to your Pixiv account through this browser window.
    * Upon successful login, `gppt` will print your `access_token` and `refresh_token` to the command line. Copy the `refresh_token`.
3.  **Run the login command (Headless - Alternative):**
    If you prefer a headless browser (no visible window):
    ```bash
    gppt login-headless -u YOUR_PIXIV_USERNAME_OR_EMAIL -p YOUR_PIXIV_PASSWORD
    ```
    * Replace placeholders with your actual Pixiv credentials.
    * Copy the `refresh_token` printed upon success.

### Downloading Images and Metadata

The `vivian_downloader.py` script handles the downloading process.

* **Functionality:** It uses the provided `refresh_token` to authenticate with the Pixiv API via `pixivpy`. It searches for illustrations tagged with tags like "ビビアン(ゼンゼロ)" (Vivian ZenZero), "ビビアン", "Vivian", "ビビアン(ゼンゼロ)", "Vivian (Zenless Zone Zero)", "ビビアン・バンシー", "Vivian Banshee", "vivian","薇薇安" and downloads both the image files and their detailed metadata (including tags like 'R-18').
* **Authentication:** You need to paste your obtained `refresh_token` into the `_REFRESH_TOKEN` variable within the script before running it.
* **Output:**
    * Images are saved into the `vivian_images/` directory.
    * Metadata for all downloaded images is saved into `vivian_tags.json`.

---

## Data Preprocessing

After downloading, the images undergo filtering and face cropping.

### SFW/NSFW Filtering

The `separate_nsfw_vivian.py` script categorizes the downloaded images.

* **Functionality:** It reads the `vivian_tags.json` file generated by the downloader. For each image entry, it checks if the tag list contains 'R-18'. If the 'R-18' tag is present, the corresponding image file is moved from the `vivian_images/` directory to a new `vivian_nsfw/` directory.
* **Purpose:** This separation allows for using both SFW and NSFW source material (specifically faces) for training the GAN while keeping the original files organized.

### Face Detection and Cropping (YOLOv3)

To train the GAN effectively on facial features and style, faces were detected and cropped using a pre-trained YOLOv3 model specifically designed for anime heads.

**Reference Tool:** [https://github.com/grapeot/AnimeHeadDetector](https://github.com/grapeot/AnimeHeadDetector)

**Setup:**

1.  **Clone the Repository:** Clone the detector repository *recursively* to ensure submodules are included:
    ```bash
    git clone --recursive [https://github.com/grapeot/AnimeHeadDetector](https://github.com/grapeot/AnimeHeadDetector)
    ```
2.  **Download Weights:** Download the pre-trained weights file:
    ```bash
    wget [https://lab.grapeot.me/head.weights](https://lab.grapeot.me/head.weights)
    ```
    *(Alternatively, paste the URL directly into your browser to download)*
3.  **Placement:** Place the downloaded `head.weights` file inside the cloned `AnimeHeadDetector` directory.

**Cropping Process (Pre-processing - crop_vivian_faces.py)

**Note:** This notebook (`Vivian_GAN_Training_morelatent_Continue_training.ipynb`) assumes images are already cropped and prepared using crop_vivian_faces.py. It loads pre-cropped face images from `vivian_nsfw_sfw_cropped_combined_faces/` directory after storing the face cropped images with the preprocessing program.

---

## DCGAN Implementation

A Deep Convolutional Generative Adversarial Network (DCGAN) was implemented from scratch using TensorFlow/Keras with checkpoint resumption capability.

### Architecture Overview

The DCGAN consists of two competing neural networks designed to operate on **128x128 pixel RGB images**:

1.  **Generator (G):** Takes a random noise vector (latent vector) as input and generates a realistic 128x128 RGB image of Vivian's face.
2.  **Discriminator (D):** Takes a 128x128 RGB image (either real from the dataset or fake from the Generator) and classifies it as real or fake.

---

### Generator Architecture

* **Input:** A latent vector (random noise) of size **512** (`LATENT_DIM = 512`)
* **Architecture (for 128x128 output):**
    * **Dense Layer:** Projects latent vector to 4×4×1024 feature map, `use_bias=False`
    * **Reshape:** To (4, 4, 1024)
    * **Upsampling Blocks (5 blocks total):**
        * Block 1: Conv2DTranspose(512, kernel_size=4, strides=2) → 8×8×512
        * Block 2: Conv2DTranspose(256, kernel_size=4, strides=2) → 16×16×256
        * Block 3: Conv2DTranspose(128, kernel_size=4, strides=2) → 32×32×128
        * Block 4: Conv2DTranspose(64, kernel_size=4, strides=2) → 64×64×64
        * Block 5: Conv2DTranspose(3, kernel_size=4, strides=2) → 128×128×3
    * Each upsampling block (except output) includes:
        * `BatchNormalization(momentum=0.9)`
        * `ReLU` activation
        * `use_bias=False`
* **Output:** 128×128×3 RGB image
* **Output Activation:** `tanh` (scales output to [-1, 1])

---

### Discriminator Architecture

* **Input:** 128×128×3 RGB image
* **Architecture (for 128x128 input):**
    * **Downsampling Blocks (5 blocks total):**
        * Block 1: Conv2D(64, kernel_size=4, strides=2) → 64×64×64
        * Block 2: Conv2D(128, kernel_size=4, strides=2) → 32×32×128
        * Block 3: Conv2D(256, kernel_size=4, strides=2) → 16×16×256
        * Block 4: Conv2D(512, kernel_size=4, strides=2) → 8×8×512
        * Block 5: Conv2D(1024, kernel_size=4, strides=2) → 4×4×1024
    * Each downsampling block includes:
        * `LeakyReLU(alpha=0.2)` activation
        * `Dropout(0.3)` regularization
        * `BatchNormalization()` (applied in blocks 2-5)
    * **Flatten** layer
    * **Dense(1)** output layer
* **Output:** Single scalar probability
* **Output Activation:** `sigmoid` (outputs probability between 0 and 1)

---
### Training Process
    * Below is a detailed explanation for Training process that is implemented 

### Data Loading And Preprocessing

* **Input Data:** Pre-cropped face images from `vivian_nsfw_sfw_cropped_combined_faces/` directory
* **Data Generator:** Custom `SingleClassDataGenerator` class (extends `keras.utils.Sequence`)
* **Preprocessing Pipeline (in `_generate_batch` method):**
    1.  **Load Image:** Using PIL (`Image.open(...).convert('RGB')`)
    2.  **Smart Resizing:**
        * If image dimensions are within tolerance (±89 pixels of 128): Direct resize to 128×128 using LANCZOS
        * Otherwise: Center crop to square, then resize to 128×128
    3.  **Random Augmentations (30% probability each):**
        * Horizontal flip
        * Brightness adjustment (0.9-1.1 range)
        * Contrast adjustment (0.9-1.1 range)
    4.  **Normalization:**
        * Convert to float32 and divide by 255.0 → [0, 1]
        * Apply formula: `(img_array - 0.5) / 0.5` → [-1, 1]
* **Batch Loading:** Images loaded in batches with error handling and automatic replacement of corrupted files
* **Max Images:** Configurable limit (`MAX_IMAGES_PER_CLASS = 10000`)

---

### Training Loop Details

* **Implementation:** Custom `ImprovedDCGAN` class with `train_step` method using `tf.GradientTape`
* **Adversarial Training Process:**
    
    1.  **Train Discriminator:**
        * Evaluate on real images (label: 0.9 with smoothing)
        * Generate fake images from random noise
        * Evaluate on fake images (label: 0.1 with smoothing)
        * Loss = (real_loss + fake_loss) / 2
        * Apply gradients to Discriminator only
    
    2.  **Train Generator:**
        * Generate new batch of fake images
        * Get Discriminator predictions (target: 1.0 to fool discriminator)
        * Calculate loss using BinaryCrossentropy
        * Apply gradients to Generator only

* **Loss Function:** `BinaryCrossentropy(from_logits=False, label_smoothing=0.1)`
* **Optimizers:**
    * **Generator:** `Adam(lr=0.0001, beta_1=0.5, beta_2=0.999)`
    * **Discriminator:** `Adam(lr=0.0004, beta_1=0.5, beta_2=0.999)`
    * Note: Discriminator has 4× higher learning rate

---

### Checkpoint Resumption System

* **Checkpoint Detection:** Automatically scans `checkpoints_128x128/vivian/` for latest saved models
* **Resume Logic:**
    * Finds latest epoch with both generator and discriminator weights
    * Loads models using `keras.models.load_model()`
    * Resumes training from next epoch
* **Checkpoint Naming:** `generator2_epoch_{epoch:03d}.keras`, `discriminator2_epoch_{epoch:03d}.keras`
* **Fallback:** If no checkpoints found, starts training from scratch

---

### Hyperparameters

* **Image Size:** 128×128 pixels
* **Batch Size:** 8
* **Total Epochs:** 400 (`EPOCHS_PER_CLASS = 400`)
* **Latent Dimension:** 512
* **Learning Rates:**
    * Generator: 0.0001
    * Discriminator: 0.0004
* **Adam Parameters:**
    * beta_1: 0.5
    * beta_2: 0.999
* **Label Smoothing:** 0.1
* **Dropout Rate:** 0.3
* **Batch Normalization Momentum:** 0.9
* **Image Crop Tolerance:** ±89 pixels

---

### Saving & Monitoring

* **Checkpoint Frequency:** Every 30 epochs
    * Saves to: `checkpoints_128x128/vivian/`
    * Format: `.keras` files
* **Sample Image Generation:** Every 5 epochs
    * Generates 8 sample images in 2×4 grid
    * Saves to: `Vivian_128x128_results/new_generated_samples_vivian_epoch_{epoch}.png`
* **Comparison Images:** Every 10 epochs
    * Shows 8 real vs 8 generated images
    * Saves to: `Vivian_128x128_results/comparison_vivian_epoch_{epoch}.png`
* **Final Model:** Saved as `vivianxsp2GenNet.keras` at training completion
* **Progress Tracking:** Real-time progress bars using `tqdm` with loss metrics

---

### Early Stopping Conditions

Training automatically stops if any of these conditions are met:

1. **Discriminator Collapse:** `avg_d_loss < 0.01` (discriminator too strong)
2. **Generator Failure:** `avg_g_loss > 6.0` (generator performing very poorly)
3. **Loss Spike Detection:** If recent generator loss (5-epoch moving average) suddenly doubles, indicating training instability

---

### Error Handling & Robustness

* **Corrupted Image Handling:** Automatic detection and replacement with random valid image
* **Multi-format Support:** Handles PNG, JPG, JPEG files (case-insensitive)
* **Max Retry Logic:** Up to `len(batch) * 2` attempts to load valid images per batch slot
* **Critical Failure Detection:** Logs when unable to load any valid image after max retries
---

## Results & Sample Outputs

The training successfully converged, resulting in a Generator capable of producing unique portraits of Vivian in a consistent, painterly art style. The model learned key facial features and rendered them with an impressionistic quality.

**Sample Generations:**

How the generations differ over epochs:

<img width="1754" height="888" alt="new_generated_samples_vivian_epoch_60" src="https://github.com/user-attachments/assets/e9d8876f-b14b-438c-8afa-70a181f80985" />

<img width="1754" height="888" alt="new_generated_samples_vivian_epoch_105" src="https://github.com/user-attachments/assets/67f7cdd8-c33b-4aa2-9c51-d40478827535" />

<img width="1754" height="888" alt="new_generated_samples_vivian_epoch_230" src="https://github.com/user-attachments/assets/634e1b34-24b1-4a96-b49f-daffb38bb743" />

<img width="1754" height="888" alt="new_generated_samples_vivian_epoch_350" src="https://github.com/user-attachments/assets/12fc75da-b40b-4405-a077-2f77d08f85b2" />



*(More better sample images can be generated after training the models)*

---

## How to Use

1.  **Setup:** Clone this repository and install the required Python libraries (TensorFlow, NumPy, Pillow, pixivpy, tqdm).
2.  **Get Pixiv Token:** Follow the steps in the [Getting a Pixiv Refresh Token](#getting-a-pixiv-refresh-token) section and update `vivian_downloader.py` with your token.
3.  **Download Data:** Run `python vivian_downloader.py` to download images and metadata.
4.  **Filter NSFW:** Run `python separate_nsfw_vivian.py` to move R-18 images.
5.  **Setup Face Detector:** Clone the `AnimeHeadDetector` repository and download its weights as described in the [Face Detection and Cropping](#face-detection-and-cropping-yolov3) section. Ensure the paths in the GAN training notebooks point correctly to this tool and its weights file.
6.  **Prepare Training Data:** Open one of the `Vivian_GAN_Training*.ipynb` notebooks (e.g., `Vivian_GAN_Training_morelatent.ipynb`). Run the cell containing the `crop_face_yolov3` function call to generate the `vivian_cropped/` directory. **Ensure paths are correct before running.**
7.  **Train GAN:** Execute the remaining cells in the Jupyter notebook to train the DCGAN. Monitor the saved sample images for progress. Use the `_Continue_training` notebook to resume from a checkpoint.
8.  **Generate Images:** Once trained, you can load the saved Generator model from a checkpoint and use it to generate new images by feeding it random latent vectors.

---

## Code Structure

* `vivian_downloader.py`: Script to download Vivian images and tags from Pixiv.
* `vivian_tags.json`: JSON file storing metadata for downloaded images (generated by downloader).
* `separate_nsfw_vivian.py`: Script to filter and move NSFW images based on tags.
* `Vivian_GAN_Training.ipynb`: Jupyter notebook for training the DCGAN (Latent Dim = 100). Includes face cropping function.
* `Vivian_GAN_Training_morelatent.ipynb`: Jupyter notebook for training the DCGAN (Latent Dim = 128). Includes face cropping function.
* `Vivian_GAN_Training_morelatent_Continue_training.ipynb`: Notebook to continue training the 128-latent-dim model from a saved checkpoint.
* `vivian_images/`: Directory where SFW images are initially downloaded.
* `vivian_nsfw/`: Directory where NSFW images are moved.
* `vivian_cropped/`: Directory where cropped faces (used for training) are saved.
* `checkpoints/`: Directory where trained model weights (Generator & Discriminator) are saved.
* *(Sample image files generated during training might also be present)*


---

## References & Tools

* **Pixiv API Wrapper:** `pixivpy` - [https://github.com/upbit/pixivpy](https://github.com/upbit/pixivpy)
* **Pixiv Refresh Token Getter:** `gppt` - [https://github.com/eggplants/get-pixivpy-token](https://github.com/eggplants/get-pixivpy-token)
* **Anime Head Detector (YOLOv3):** `AnimeHeadDetector` - [https://github.com/grapeot/AnimeHeadDetector](https://github.com/grapeot/AnimeHeadDetector)
* **DCGAN Paper (Reference):** Radford, A., Metz, L., & Chintala, S. (2015). Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. *arXiv preprint arXiv:1511.06434*.
