# Generative-Character-An-Artistic-Interpretation

# Vivian (Zenless Zone Zero) - GAN Portrait Generation

This repository contains a from-scratch GAN implementation trained to generate unique, artistic portraits of the character "Vivian" from the game Zenless Zone Zero.

The goal was to develop a model that could create a consistent and compelling art style. The network successfully converged on a painterly aesthetic, learning to render the character's key features with an impressionistic quality.



---

## Table of Contents

* [Project Goal & Overview](#project-goal--overview)
* [Data Acquisition (Pixiv Scraping)](#data-acquisition-pixiv-scraping)
    * [Getting a Pixiv Refresh Token](#getting-a-pixiv-refresh-token)
    * [Downloading Images and Metadata](#downloading-images-and-metadata)
* [Data Preprocessing](#data-preprocessing)
    * [SFW/NSFW Filtering](#sfwnsfw-filtering)
    * [Face Detection and Cropping (YOLOv3)](#face-detection-and-cropping-yolov3)
* [DCGAN Implementation](#dcgan-implementation)
    * [Architecture Overview](#architecture-overview)
    * [Generator](#generator)
    * [Discriminator](#discriminator)
* [Training Process](#training-process)
    * [Data Loading & Preprocessing](#data-loading--preprocessing-for-gan)
    * [Training Loop Details](#training-loop-details)
    * [Hyperparameters & Variations](#hyperparameters--variations)
* [Results & Sample Outputs](#results--sample-outputs)
* [How to Use](#how-to-use)
* [Code Structure](#code-structure)
* [References & Tools](#references--tools)

---

## Project Goal & Overview

The primary objective was to train a Deep Convolutional Generative Adversarial Network (DCGAN) capable of generating novel, high-quality artistic portraits of Vivian. The process involved several stages:

1.  **Scraping Data:** Downloading a comprehensive set of images featuring Vivian, along with their associated metadata (tags), from Pixiv.
2.  **Filtering Data:** Separating Safe-For-Work (SFW) images from Not-Safe-For-Work (NSFW) ones based on Pixiv tags.
3.  **Detecting & Cropping Faces:** Using a pre-trained YOLOv3 model specialized for anime heads to isolate Vivian's face from both SFW and NSFW images. This ensures the GAN focuses solely on learning facial features and artistic style.
4.  **Training the DCGAN:** Feeding the cropped facial images into the DCGAN for training, allowing it to learn the underlying patterns and generate new faces in a similar, yet unique, painterly style.

---

## Data Acquisition (Pixiv Scraping)

Images and metadata were sourced from Pixiv using the `pixivpy` library. This requires authentication via a refresh token.

### Getting a Pixiv Refresh Token

To use the Pixiv API, you need a `refresh_token`. The `gppt` tool simplifies this process.

**Reference:** [https://github.com/eggplants/get-pixivpy-token](https://github.com/eggplants/get-pixivpy-token)

**Prerequisites:**
* Google Chrome browser installed.
* Selenium and the appropriate WebDriver for your Chrome version.

**Steps:**

1.  **Install `gppt`:**
    ```bash
    pip install gppt
    ```
2.  **Run the login command (Interactive):**
    ```bash
    gppt login
    ```
    * This will launch a Chrome browser window.
    * Log in to your Pixiv account through this browser window.
    * Upon successful login, `gppt` will print your `access_token` and `refresh_token` to the command line. Copy the `refresh_token`.
3.  **Run the login command (Headless - Alternative):**
    If you prefer a headless browser (no visible window):
    ```bash
    gppt login-headless -u YOUR_PIXIV_USERNAME_OR_EMAIL -p YOUR_PIXIV_PASSWORD
    ```
    * Replace placeholders with your actual Pixiv credentials.
    * Copy the `refresh_token` printed upon success.

### Downloading Images and Metadata

The `vivian_downloader.py` script handles the downloading process.

* **Functionality:** It uses the provided `refresh_token` to authenticate with the Pixiv API via `pixivpy`. It searches for illustrations tagged with tags like "ビビアン(ゼンゼロ)" (Vivian ZenZero), "ビビアン", "Vivian", "ビビアン(ゼンゼロ)", "Vivian (Zenless Zone Zero)", "ビビアン・バンシー", "Vivian Banshee", "vivian","薇薇安" and downloads both the image files and their detailed metadata (including tags like 'R-18').
* **Authentication:** You need to paste your obtained `refresh_token` into the `_REFRESH_TOKEN` variable within the script before running it.
* **Output:**
    * Images are saved into the `vivian_images/` directory.
    * Metadata for all downloaded images is saved into `vivian_tags.json`.

---

## Data Preprocessing

After downloading, the images undergo filtering and face cropping.

### SFW/NSFW Filtering

The `separate_nsfw_vivian.py` script categorizes the downloaded images.

* **Functionality:** It reads the `vivian_tags.json` file generated by the downloader. For each image entry, it checks if the tag list contains 'R-18'. If the 'R-18' tag is present, the corresponding image file is moved from the `vivian_images/` directory to a new `vivian_nsfw/` directory.
* **Purpose:** This separation allows for using both SFW and NSFW source material (specifically faces) for training the GAN while keeping the original files organized.

### Face Detection and Cropping (YOLOv3)

To train the GAN effectively on facial features and style, faces were detected and cropped using a pre-trained YOLOv3 model specifically designed for anime heads.

**Reference Tool:** [https://github.com/grapeot/AnimeHeadDetector](https://github.com/grapeot/AnimeHeadDetector)

**Setup:**

1.  **Clone the Repository:** Clone the detector repository *recursively* to ensure submodules are included:
    ```bash
    git clone --recursive [https://github.com/grapeot/AnimeHeadDetector](https://github.com/grapeot/AnimeHeadDetector)
    ```
2.  **Download Weights:** Download the pre-trained weights file:
    ```bash
    wget [https://lab.grapeot.me/head.weights](https://lab.grapeot.me/head.weights)
    ```
    *(Alternatively, paste the URL directly into your browser to download)*
3.  **Placement:** Place the downloaded `head.weights` file inside the cloned `AnimeHeadDetector` directory.

**Cropping Process (Implemented in GAN Training Notebooks):**

* The Jupyter notebooks (`Vivian_GAN_Training*.ipynb`) contain a `crop_face_yolov3` function.
* **Functionality:**
    * This function takes an input image directory (e.g., `vivian_images/` or `vivian_nsfw/`), an output directory (`vivian_cropped/`), the path to the `AnimeHeadDetector` directory, and the path to the `head.weights` file.
    * It iterates through images in the input directory.
    * For each image, it uses the `AnimeHeadDetector`'s functionality (specifically `AnimeHeadDetection.py`) to detect faces.
    * If a face is detected, it crops the bounding box area from the original image.
    * The cropped face image is then saved to the specified output directory (`vivian_cropped/`).
* **Execution:** This cropping function is typically run once at the beginning of the GAN training notebooks to prepare the dataset of faces. It processes images from both the SFW (`vivian_images/`) and NSFW (`vivian_nsfw/`) directories, consolidating all cropped faces into `vivian_cropped/`.

---

## DCGAN Implementation

A Deep Convolutional Generative Adversarial Network (DCGAN) was implemented from scratch using TensorFlow/Keras.

### Architecture Overview

The DCGAN consists of two competing neural networks designed to operate on **128x128** or **256x256** pixel images:

1.  **Generator (G):** Takes a random noise vector (latent vector) as input and attempts to generate a realistic image of Vivian's face at the target resolution (128x128 or 256x256).
2.  **Discriminator (D):** Takes an image (either real from the dataset or fake from the Generator) at the target resolution and tries to classify it as real or fake.

---
### Generator

* **Input:** A latent vector (random noise) of size `LATENT_DIM`. Experiments used dimensions of **100** (`Vivian_GAN_Training.ipynb`) and **512** (`Vivian_GAN_Training_morelatent*.ipynb`).
* **Layers (Example for 128x128 output, details vary slightly for 256x256):**
    * A `Dense` layer projects the latent vector into a higher-dimensional space and reshapes it into an initial small feature map (e.g., 4x4x1024). `use_bias=False` is used, often paired with Batch Normalization.
    * Multiple blocks of `Conv2DTranspose` (Transposed Convolution) layers with `strides=2` are used to progressively upsample the spatial dimensions (4x4 -> 8x8 -> 16x16 -> 32x32 -> 64x64 -> 128x128). The number of filters typically decreases in each upsampling step (e.g., 1024 -> 512 -> 256 -> 128 -> 64). Kernel size is often 5x5.
    * `BatchNormalization` is applied after `Conv2DTranspose` layers (except the output layer) to help stabilize training. `momentum=0.8` is commonly used.
    * `LeakyReLU` activation function (`alpha=0.2`) is used in the hidden layers.
* **Output:** A tensor representing the generated image with 3 color channels (RGB) at the target resolution (e.g., 128x128x3).
* **Output Activation:** `tanh` activation function, scaling the output pixel values to the range [-1, 1].

---
### Discriminator

* **Input:** An image (real or fake) with 3 color channels at the target resolution (e.g., 128x128x3).
* **Layers (Example for 128x128 input, details vary slightly for 256x256):**
    * A series of `Conv2D` layers with `strides=2` progressively downsample the input image into smaller, deeper feature maps (e.g., 128x128 -> 64x64 -> 32x32 -> 16x16 -> 8x8 -> 4x4). The number of filters typically increases (e.g., 64 -> 128 -> 256 -> 512 -> 1024). Kernel size is often 5x5.
    * `LeakyReLU` activation function (`alpha=0.2`) is used after convolutional layers.
    * `Dropout` (rate typically 0.3) is applied after activations in several layers for regularization.
    * `BatchNormalization` is used in later convolutional blocks (`momentum=0.8`).
    * A `Flatten` layer converts the final feature map (e.g., 4x4x1024) into a 1D vector.
* **Output:** A single scalar value.
* **Output Activation:** `sigmoid` activation function, outputting a probability (between 0 and 1) indicating whether the input image is classified as real.

---

### Data Loading & Preprocessing (for GAN)

* **Input Data:** The training process uses the cropped face images stored in the `vivian_cropped/` directory.
* **Preprocessing Steps (within  `preprocess_image` function in notebooks):**
    1.  **Load Image:** Images are loaded using PIL (`Image.open`).
    2.  **Find Biggest Square:** A function takes an image and a `mask_threshold` (e.g., 245). It converts the image to grayscale and identifies non-background pixels (pixels darker than the threshold). It then finds the largest possible square bounding box containing these non-background pixels, potentially allowing some tolerance (`tolerance` parameter) to include slightly more area around the main subject. This helps center the face and remove excess background before resizing.
    3.  **Crop Square:** The image is cropped to the determined square bounding box.
    4.  **Resize:** The cropped square image is resized to the target dimensions (`IMAGE_SIZE` x `IMAGE_SIZE`, e.g., 128x128 or 256x256) using `Image.Resampling.LANCZOS` for high-quality downsampling.
    5.  **Convert & Normalize:** The resized image is converted to a NumPy array (`float32`), and pixel values are normalized from the [0, 255] range to the **[-1, 1]** range. This is done by dividing by 127.5 and subtracting 1.0.
* **Data Loading:** A custom data generator (`DataGenerator`) or `tf.keras.utils.image_dataset_from_directory` loads these preprocessed images in batches.
* **Augmentation:** Random horizontal flipping (`tf.image.random_flip_left_right`) is applied during training to augment the dataset.

---
### Training Loop Details

* **Adversarial Training:** Implemented within the `train_step` method of a custom `tf.keras.Model` subclass (`DCGAN`). The loop alternates between training the Discriminator and the Generator using `tf.GradientTape`.
    1.  **Train Discriminator:**
        * Takes a batch of real images (preprocessed cropped faces).
        * Generates a batch of fake images using the Generator from random noise.
        * Calculates the Discriminator's loss on real images (target label: 1) and fake images (target label: 0) using `BinaryCrossentropy` (`from_logits=False` because of the sigmoid output). Label smoothing (e.g., using 0.9 for real labels) might be applied.
        * The total discriminator loss is the average of the real and fake losses.
        * Gradients are computed and applied to the Discriminator's weights using its optimizer.
    2.  **Train Generator:**
        * Generates a *new* batch of fake images from random noise.
        * Calculates the Generator's loss based on the Discriminator's prediction for these fake images (target label: 1, aiming to fool the discriminator) using `BinaryCrossentropy`.
        * Gradients are computed *only* for the Generator's weights and applied using its optimizer.
* **Loss Function:** `tf.keras.losses.BinaryCrossentropy` is used for both networks.
* **Optimizers:** `tf.keras.optimizers.Adam` is used for both the Generator (`generator_optimizer`) and Discriminator (`discriminator_optimizer`), configured with a specific learning rate (e.g., 0.0001 or 0.0002) and `beta_1` (e.g., 0.5).

---
### Hyperparameters & Variations

* **Image Size:** Experiments run with `IMAGE_SIZE` set to **128** (`Vivian_GAN_Training.ipynb`) and **256** (`Vivian_GAN_Training_morelatent*.ipynb`).
* **Batch Size:** Typically 64.
* **Epochs:** Trained for 300+ epochs, with notebooks available for continuing training (`Vivian_GAN_Training_morelatent_Continue_training.ipynb`).
* **Latent Dimension (`LATENT_DIM`):** Experiments used **100** and **512**.
* **Learning Rate:** Often set around 0.0001 or 0.0002.
* **Adam Beta_1:** Set to 0.5.
* **Checkpoints:** Model weights (Generator and Discriminator) are saved periodically (e.g., every 50 epochs) into a `checkpoints/` directory using `generator.save_weights` and `discriminator.save_weights`.
* **Sample Generation:**
    * Sample images (`generate_and_save_images`) generated by the current Generator state are saved at regular intervals (e.g., every 10 epochs) to visually monitor progress.
    * Comparison images (`generate_and_save_comparison_images`) showing real images alongside generated images are saved separately every few epochs (e.g., every 50 epochs).

---

## Results & Sample Outputs

The training successfully converged, resulting in a Generator capable of producing unique portraits of Vivian in a consistent, painterly art style. The model learned key facial features and rendered them with an impressionistic quality.

**Sample Generations:**

How the generations differ over epochs:

<img width="1754" height="888" alt="new_generated_samples_vivian_epoch_60" src="https://github.com/user-attachments/assets/e9d8876f-b14b-438c-8afa-70a181f80985" />

<img width="1754" height="888" alt="new_generated_samples_vivian_epoch_105" src="https://github.com/user-attachments/assets/67f7cdd8-c33b-4aa2-9c51-d40478827535" />

<img width="1754" height="888" alt="new_generated_samples_vivian_epoch_230" src="https://github.com/user-attachments/assets/634e1b34-24b1-4a96-b49f-daffb38bb743" />

<img width="1754" height="888" alt="new_generated_samples_vivian_epoch_350" src="https://github.com/user-attachments/assets/12fc75da-b40b-4405-a077-2f77d08f85b2" />



*(More better sample images can be generated after training the models)*

---

## How to Use

1.  **Setup:** Clone this repository and install the required Python libraries (TensorFlow, NumPy, Pillow, pixivpy, tqdm).
2.  **Get Pixiv Token:** Follow the steps in the [Getting a Pixiv Refresh Token](#getting-a-pixiv-refresh-token) section and update `vivian_downloader.py` with your token.
3.  **Download Data:** Run `python vivian_downloader.py` to download images and metadata.
4.  **Filter NSFW:** Run `python separate_nsfw_vivian.py` to move R-18 images.
5.  **Setup Face Detector:** Clone the `AnimeHeadDetector` repository and download its weights as described in the [Face Detection and Cropping](#face-detection-and-cropping-yolov3) section. Ensure the paths in the GAN training notebooks point correctly to this tool and its weights file.
6.  **Prepare Training Data:** Open one of the `Vivian_GAN_Training*.ipynb` notebooks (e.g., `Vivian_GAN_Training_morelatent.ipynb`). Run the cell containing the `crop_face_yolov3` function call to generate the `vivian_cropped/` directory. **Ensure paths are correct before running.**
7.  **Train GAN:** Execute the remaining cells in the Jupyter notebook to train the DCGAN. Monitor the saved sample images for progress. Use the `_Continue_training` notebook to resume from a checkpoint.
8.  **Generate Images:** Once trained, you can load the saved Generator model from a checkpoint and use it to generate new images by feeding it random latent vectors.

---

## Code Structure

* `vivian_downloader.py`: Script to download Vivian images and tags from Pixiv.
* `vivian_tags.json`: JSON file storing metadata for downloaded images (generated by downloader).
* `separate_nsfw_vivian.py`: Script to filter and move NSFW images based on tags.
* `Vivian_GAN_Training.ipynb`: Jupyter notebook for training the DCGAN (Latent Dim = 100). Includes face cropping function.
* `Vivian_GAN_Training_morelatent.ipynb`: Jupyter notebook for training the DCGAN (Latent Dim = 128). Includes face cropping function.
* `Vivian_GAN_Training_morelatent_Continue_training.ipynb`: Notebook to continue training the 128-latent-dim model from a saved checkpoint.
* `vivian_images/`: Directory where SFW images are initially downloaded.
* `vivian_nsfw/`: Directory where NSFW images are moved.
* `vivian_cropped/`: Directory where cropped faces (used for training) are saved.
* `checkpoints/`: Directory where trained model weights (Generator & Discriminator) are saved.
* *(Sample image files generated during training might also be present)*

---

## References & Tools

* **Pixiv API Wrapper:** `pixivpy` - [https://github.com/upbit/pixivpy](https://github.com/upbit/pixivpy)
* **Pixiv Refresh Token Getter:** `gppt` - [https://github.com/eggplants/get-pixivpy-token](https://github.com/eggplants/get-pixivpy-token)
* **Anime Head Detector (YOLOv3):** `AnimeHeadDetector` - [https://github.com/grapeot/AnimeHeadDetector](https://github.com/grapeot/AnimeHeadDetector)
* **DCGAN Paper (Reference):** Radford, A., Metz, L., & Chintala, S. (2015). Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. *arXiv preprint arXiv:1511.06434*.
